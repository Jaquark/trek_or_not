{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 4",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaquark/trek_or_not/blob/master/Project_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsALbdJ9kzhq",
        "colab_type": "text"
      },
      "source": [
        "I am a huge fan of Star Trek. I am also a huge fan of giving up a lot of my agency to machines to make my life easier; so why not make a naive bayesian classifier to find out whether or not a television show is Star Trek or not based on plot summaries of the episodes? Because there are better ways to tell if a television show is Star Trek or not - like watching it, or simply reading the title; however the concept of doing it is still a valid way of writing and testing a naive bayesian classifier, which is exactly what we will be doing:\n",
        "\n",
        "Is it Star Trek (interesting) or Not?\n",
        "\n",
        "\n",
        "As a bit of an aside, the datasets will be working with will/can be quite large and my computer is quite slow (First generation Pentium i3, an i3-530 at that), so I am going to farm out the processing to Google using this Jupyter notebook and setting up a pyspark cluster.\n",
        "\n",
        "For the best experience, you can run the entire runtime in a straight go by going to Runtime --> Restar and Run All, or by pressing ctrl+M followed by ctrl+F9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdVdq9vOkhxe",
        "colab_type": "code",
        "outputId": "f435ab8d-69c2-4431-fdb4-75cc46a1dbce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!rm -r spark*\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz     \n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!curl https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt > most_common_words.txt\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'spark*': No such file or directory\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75153  100 75153    0     0   365k      0 --:--:-- --:--:-- --:--:--  365k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4eDgNTZmjmh",
        "colab_type": "text"
      },
      "source": [
        "Additionally, we will need access to the wikipedia API; installing, via pip, the module wikipedia will allow us simple parsing;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AlQn4cvmq2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a883fb6-b03c-4947-c5b2-643d5f850963"
      },
      "source": [
        "!pip install -q wikipedia\n",
        "import wikipedia\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import string\n",
        "import math\n",
        "import shutil\n",
        "conf=SparkConf().setAppName(\"interesting\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdVuHantm5WA",
        "colab_type": "text"
      },
      "source": [
        "Next we will have some (hardcoded) list of episodes of various TV shows, with a training set of Star Trek episodes (as well as another set of Star Trek episodes to not be in the training set)\n",
        "\n",
        "I'm going to start with just epsidoes from The Original Series, The Animated Series and The Next Generation; and hope with a limited dataset we can find other star trek episodes from other series.\n",
        "\n",
        "To me, I am defining that which is interesting as that which is Star Trek. Television shows which have a lot of references to science fiction are more difficult to classify as they have similar sematics. Shows that are homages are incredibly difficult to classify - as in the Futurama episode \"Where No Fan Has Gone Before\", which is contains all of the original crew of Star Trek except James Doohan, who thought it was stupid, and DeForest Kelley, who was dead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcZsTpQJnD3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_star_trek = [\"The Man Trap\", \t\"A Private Little War\", \t\"Whom Gods Destroy (Star Trek: The Original Series)\", #TOS\n",
        "                          \"The Infinite Vulcan\", #TAS\n",
        "                          \t\"Code of Honor (Star Trek: The Next Generation)\", \t\"When the Bough Breaks (Star Trek: The Next Generation)\", #TNG S1\n",
        "                            \t\"A Matter of Honor (Star Trek: The Next Generation)\", \"The Measure of a Man (Star Trek: The Next Generation)\", #S2\n",
        "                              \"Booby Trap (Star Trek: The Next Generation)\", \"Yesterday's Enterprise (Star Trek: The Next Generation)\", #S3\n",
        "                            \"Future Imperfect (Star Trek: The Next Generation)\", \t\"Clues (Star Trek: The Next Generation)\", #S4\n",
        "                          \t\"Darmok (Star Trek: The Next Generation)\", \"The Game (Star Trek: The Next Generation)\",\n",
        "                            \"Phantasms  (Star Trek: The Next Generation)\", \"The Pegasus (Star Trek: The Next Generation)\",\n",
        "                          \"Frame of Mind (Star Trek: The Next Generation)\", \"Ship in a Bottle (Star Trek: The Next Generation)\"\n",
        "                         ]\n",
        "\n",
        "training_set_is_not_star_trek = [\"Hell Is Other Robots\", \"Fry and the Slurm Factory\", \"The 30% Iron Chef\", \"The Honking\", \"That's Lobstertainment!\", \"A Head in the Polls\",\n",
        "                                \"Time-Flight\", \"The King's Demons\",\"Childhood's End (Stargate Atlantis)\", \"33 (Battlestar Galactica)\", \"Colonial Day\", \"The Joining (The Outer Limits)\"\n",
        "                                ,\"Resurrection (The Outer Limits)\", \"The Tagger\", \"Enemies (Stargate SG-1)\", \"Downloaded (Battlestar Galactica)\",\n",
        "                                \"R2 Come Home/Lethal Trackdown\"]\n",
        "\n",
        "set_to_use = { 'trek' : training_set_star_trek, 'not_trek' : training_set_is_not_star_trek }\n",
        "\n",
        "\n",
        "unknown_but_trek = [\"Equilibrium (Star Trek: Deep Space Nine)\", \"Context Is for Kings\"]\n",
        "\n",
        "unkown_but_not_trek = [\"When Aliens Attack\", \"Children of the Gods\", \"Full Boyle\", \"Where No Fan Has Gone Before\",\n",
        "                      ]\n",
        "\n",
        "prediction_set = { 'trek' : unknown_but_trek, 'not_trek' : unkown_but_not_trek }\n",
        "\n",
        "\n",
        "first_500 = []\n",
        "with open('most_common_words.txt') as file:\n",
        "  first_500 = [next(file).replace('\\n','') for x in range(500)]\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6_PcqBBqGMr",
        "colab_type": "text"
      },
      "source": [
        "(It took a few runs of the previous and the next block to find out the proper names of the episodes according to wikipedia, copy and paste was not my friend)\n",
        "\n",
        "Now we want to scrape the plot section of all of those wikipedia pages, additionally we make a corresponding array (for training_purposes) that tells whether or not the plot summary is Star Trek related or not.\n",
        "\n",
        "The Scrape_Data function has its limitations, as not all television shows posted on Wikipedia share the same page format. Some episodes use the word 'Plot', others use the word 'Synopsis'. All television shows, thus, will be shows that use 'Plot' instead of 'Synopsis'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6JQV4mlqNcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def scrape_data (set_to_use):\n",
        "\n",
        "  plot_synopsis = []\n",
        "  is_trek = []\n",
        "  episode_name = []\n",
        "  \n",
        "  for k in set_to_use.keys():\n",
        "    for e in set_to_use[k]:\n",
        "      if k == 'trek':\n",
        "        is_trek.append(1)\n",
        "      else :\n",
        "        is_trek.append(0)\n",
        "      \n",
        "      plot = \"\"\n",
        "      section = wikipedia.WikipediaPage(title = e).section('Plot')\n",
        "      section.replace('\\n',' ')\n",
        "      plot += section\n",
        "      #and then we do some cleanup\n",
        "      plot = plot.lower()\n",
        "      plot.replace('(','').translate(str.maketrans(\"\", \"\", string.punctuation)) \n",
        "      \n",
        "      plot_synopsis.append(plot)\n",
        "      episode_name.append(e)\n",
        "      \n",
        "  return episode_name,plot_synopsis, is_trek\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjidy_E98cK_",
        "colab_type": "text"
      },
      "source": [
        "Once we've collected the training data, in the form of a tuple of lists of the same length, we can then create our best fits. Please forgive my over use of global variables. I'm not proud of it.\n",
        "\n",
        "In the following **fit** function, we take the plot and its trekitude (that is verisimilitude of trek); wherein we calculate the percentages and log values, as well as build a corpus of all words used and the count of words for our two categories - trek or not - necessary to create a naive bayesian classifier.\n",
        "\n",
        "The work of generating the trek and non trek related words is created by sending the individual plot summaries off to a spark server running on this Colab instance; we simply take all the words and the value one as a tuple, then reduceit on the word value; that is the tuples ('the',1),('the',1);('to',1) will reduce to ('the',2),('to',1); thus giving us our counts of each word for each set.\n",
        "\n",
        "Additionally, the 500 (or so) most common English words have been stripped from both the individual counts as well as the corpus. Super common words increase the noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EbltsS68sPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "percentage_of_trek= 0\n",
        "precentage_of_non_trek = 0\n",
        "file_name = [\"trek.txt\",\"not_trek.txt\"]\n",
        "trek_training_set = {}\n",
        "not_trek_training_set = {}\n",
        "corpus = set()\n",
        "log_known_trek = 0\n",
        "log_known_non_trek = 0\n",
        "number_of_trek = 0\n",
        "number_of_not_trek = 0\n",
        "corpus = set()\n",
        "\n",
        "def fit( plot, trek):\n",
        "  global trek_training_set\n",
        "  global not_trek_training_set\n",
        "  global percentage_of_trek\n",
        "  global precentage_of_non_trek\n",
        "  global corpus\n",
        "  global log_known_trek\n",
        "  global log_known_non_trek\n",
        "  global number_of_trek\n",
        "  global number_of_not_trek\n",
        "  length = len(plot)\n",
        "  \n",
        "  #first we need to calculate \n",
        "  print(length)\n",
        "  print(trek)\n",
        "  for i in trek:\n",
        "    if i == 1:\n",
        "      number_of_trek += 1\n",
        "    else:\n",
        "      number_of_not_trek += 1\n",
        "   \n",
        "  percentage_of_trek = number_of_trek / length\n",
        "  log_known_trek = math.log(percentage_of_trek)\n",
        "  precentage_of_non_trek = number_of_not_trek / length\n",
        "  log_known_non_trek = math.log(precentage_of_non_trek)\n",
        "  \n",
        "  #This is where we should do the tokenization that we did earlier above.\n",
        "  #And use it or loose it, like before, we will be doing this with Spark, unnecessary, but it's a skill like any other\n",
        "  #continual practice makes one an expert\n",
        "  \n",
        "  \n",
        "  #build the dataset of everything we are using\n",
        "\n",
        "  for episode,is_trek in zip(plot,trek):\n",
        "    if is_trek == 1:\n",
        "      file_idx = 0\n",
        "    else :\n",
        "      file_idx = 1\n",
        "    file = '/content/{0}'.format(file_name[file_idx])\n",
        "    if os.path.exists(file):\n",
        "      append_write = 'a' # append if already exists\n",
        "    else:\n",
        "      append_write = 'w' # make a new file if not\n",
        "    \n",
        "    text_file = open(file, \"{0}+\".format(append_write))\n",
        "    text_file.write(episode)\n",
        "    text_file.write('\\n')\n",
        "    text_file.close()\n",
        "\n",
        "  for f in file_name:\n",
        "    file = '/content/{0}'.format(f)\n",
        "    tf = sc.textFile(file)\n",
        "\n",
        "    map_training_set = tf.flatMap(lambda line : line.split(\" \")).map( lambda x: (x,1))\n",
        "    map_training_set.saveAsTextFile(\"map_training_set_{0}\".format(f))\n",
        "\n",
        "    reduce_training_set = map_training_set.reduceByKey(lambda a, b : a + b)\n",
        "    reduce_training_set.saveAsTextFile(\"reduce_training_set_{0}\".format(f))\n",
        "\n",
        "    if f == 'trek.txt':\n",
        "      trek_training_set = reduce_training_set.collectAsMap()\n",
        "    else:\n",
        "      not_trek_training_set = reduce_training_set.collectAsMap()\n",
        "      \n",
        "    delta_trek = []\n",
        "    delta_not =[]\n",
        "    list_of_not_trek =  list(not_trek_training_set.keys())\n",
        "    list_of_trek = list(trek_training_set.keys())\n",
        "    \n",
        "    #this takes forever, how do we fix it?\n",
        "    for k in trek_training_set.keys():\n",
        "      corpus.add(k)\n",
        "  \n",
        "    for s in not_trek_training_set.keys():\n",
        "      if s not in list_of_trek:\n",
        "        delta_trek.append(s)\n",
        "\n",
        "    for k in not_trek_training_set.keys():\n",
        "      corpus.add(k)\n",
        "      \n",
        "    for s in trek_training_set.keys():\n",
        "      if s not in list_of_not_trek:\n",
        "        delta_not.append(s)\n",
        "         \n",
        "    for i in delta_not:\n",
        "      not_trek_training_set[i] = 0\n",
        "      \n",
        "    for i in delta_trek:\n",
        "      trek_training_set[i] = 0\n",
        "    \n",
        "    list_of_not_trek =  list(not_trek_training_set.keys())\n",
        "    list_of_trek = list(trek_training_set.keys())\n",
        "    \n",
        "    \n",
        "    #remove most common words from training sets\n",
        "    for word in first_500:\n",
        "      if word in list_of_not_trek:\n",
        "        del not_trek_training_set[word]\n",
        "      if word in list_of_trek:\n",
        "        del trek_training_set[word]\n",
        "      if word in corpus:\n",
        "        corpus.remove(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhk09qoACpdd",
        "colab_type": "text"
      },
      "source": [
        "With the functionality explained, we can now gather the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9Nq5H6CtKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_epside,training_plot,training_trek = scrape_data(set_to_use)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhrGt1iFFB3T",
        "colab_type": "text"
      },
      "source": [
        "Then we can, taking our plots and associated trekitudes, we can do the fitting, thus generating our bayesian classifier model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xfPex3IFHE5",
        "colab_type": "code",
        "outputId": "8e420ff7-b14c-438d-a299-9daa76fb1ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#each time we run to ensure data is freshly populated\n",
        "!rm -r ./map*\n",
        "!rm -r ./reduce*\n",
        "!rm -r ./trek*\n",
        "!rm -r ./not_*\n",
        "\n",
        "fit(training_plot,training_trek)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './map*': No such file or directory\n",
            "rm: cannot remove './reduce*': No such file or directory\n",
            "rm: cannot remove './trek*': No such file or directory\n",
            "rm: cannot remove './not_*': No such file or directory\n",
            "35\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLec3Kk0Y1aa",
        "colab_type": "text"
      },
      "source": [
        "With our model now created, we can predict:\n",
        "\n",
        "We take our set of values we want to test - stored in prediction_set - and we do much the same as we did with the fit function.\n",
        "\n",
        "However, this time we need to find which value is \"stronger\" - that is, the sum of all the probabilities words in a given Plot and compare which is bigger. The larger the sum, the more likely that trekitude it is - that means, if we have a value of -600 for Star Trek and a value of -760 for not Star Trek, we can be reasonably sure that it is Star Trek.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GngGjeEtY33K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def predict(plot_of_unknowns):\n",
        "  global number_of_trek\n",
        "  global number_of_not_trek\n",
        "  trek_or_not = []\n",
        "  trek_score = []\n",
        "  not_trek_score = []\n",
        "  for plot in plot_of_unknowns:\n",
        "    trek = 0\n",
        "    not_trek = 0\n",
        "    word_usage = {}\n",
        "    \n",
        "    if os.path.exists('/content/predict_map'):\n",
        "      shutil.rmtree('/content/predict_map')\n",
        "    if os.path.exists('/content/predict_reduce'):  \n",
        "      shutil.rmtree('/content/predict_reduce')\n",
        "      \n",
        "    text_file = open('test', \"w+\")\n",
        "    text_file.write(plot)\n",
        "    text_file.write('\\n')\n",
        "    text_file.close()\n",
        "    \n",
        "    tf = sc.textFile('test')\n",
        "    predict_map = tf.flatMap(lambda line : line.split(\" \")).map( lambda x: (x,1))\n",
        "    predict_map.saveAsTextFile(\"predict_map\")\n",
        "\n",
        "    predict_reduce = predict_map.reduceByKey(lambda a, b : a + b)\n",
        "    predict_reduce.saveAsTextFile(\"predict_reduce\")\n",
        "    \n",
        "    word_usage = predict_reduce.collectAsMap()\n",
        "    \n",
        "    for word in word_usage.keys():\n",
        "      \n",
        "      if word not in corpus: continue\n",
        "\n",
        "      log_trained_trek = math.log((trek_training_set[word] + 1) / (number_of_trek + len(corpus)))\n",
        "      log_trained_not_trek = math.log((not_trek_training_set[word] + 1) / (number_of_not_trek + len(corpus)))\n",
        "      \n",
        "      trek += log_trained_trek\n",
        "      not_trek += log_trained_not_trek\n",
        "      \n",
        "    trek += log_known_trek\n",
        "    not_trek += log_known_non_trek\n",
        "    \n",
        "\n",
        "    if trek > not_trek:\n",
        "      trek_or_not.append(1)\n",
        "    else:\n",
        "      trek_or_not.append(0)\n",
        "          \n",
        "    trek_score.append(trek)\n",
        "    not_trek_score.append(not_trek)\n",
        "    \n",
        "  return trek_or_not, trek_score,not_trek_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okndfYENWFxp",
        "colab_type": "text"
      },
      "source": [
        "And once all the above cells have been run, we can see what the outcome of our predictions are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX73kQnDWG-A",
        "colab_type": "code",
        "outputId": "d801f6a1-cc98-48a0-dc7f-5f9f39e7424d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "episode_prediction,prediction_plot,training_trek = scrape_data(prediction_set)\n",
        "\n",
        "predictions,trek,not_trek = predict(prediction_plot)\n",
        "\n",
        "for episode, training, prediction, ts, nts in zip(episode_prediction, training_trek, predictions,trek,not_trek):\n",
        "  statement = ''\n",
        "  if training == prediction:\n",
        "    if training == 0:\n",
        "      statement = ' was correctly classified as not Star Trek'\n",
        "    else:\n",
        "      statement = ' was correctly classified as Star Trek'\n",
        "  else:\n",
        "    statement = ''\n",
        "    if training == 0:\n",
        "      statement = ' was incorrectly classified as Star Trek'\n",
        "    else:\n",
        "      statement = ' was incorrectly classified as not Star Trek'\n",
        "      \n",
        "  print('{0}{1} with a Trek score of {2} and a Not Trek Score of {3}'.format(episode,statement,ts,nts))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Equilibrium (Star Trek: Deep Space Nine) was correctly classified as Star Trek with a Trek score of -649.9599452535407 and a Not Trek Score of -666.857315201658\n",
            "Context Is for Kings was correctly classified as Star Trek with a Trek score of -526.2954610000764 and a Not Trek Score of -543.1980174571765\n",
            "When Aliens Attack was correctly classified as not Star Trek with a Trek score of -743.800041265743 and a Not Trek Score of -738.3645966686486\n",
            "Children of the Gods was correctly classified as not Star Trek with a Trek score of -6.526711744026447 and a Not Trek Score of -6.2353262438129375\n",
            "Full Boyle was correctly classified as not Star Trek with a Trek score of -494.15062277856083 and a Not Trek Score of -471.97153317104613\n",
            "Where No Fan Has Gone Before was incorrectly classified as Star Trek with a Trek score of -592.2556046721269 and a Not Trek Score of -600.4838898387595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuFq7obfWQXo",
        "colab_type": "text"
      },
      "source": [
        "As we can see - the Deep Space 9 episode and the Discover episode ('Context is For Kings') have both been correctly identified as Star Trek.\n",
        "\n",
        "The Futurama episode, When Aliens Attack, Stargate SG-1, Children of the Gods, and Brooklyn 99, Full Boyle, epsidoes were correctly identified as not being Star Trek.\n",
        "\n",
        "And we can see the limitations with the Futurama episode, Where No Fan has gone before, which has a very large overlap with Star Trek itself - as stated earlier."
      ]
    }
  ]
}